{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../images/besm.png' width='150px'>\n",
    "<style>\n",
    "    \n",
    "@font-face {font-family: \"B Lotus\"; src: url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.eot\"); src: url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.eot?#iefix\") format(\"embedded-opentype\"), url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.woff2\") format(\"woff2\"), url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.woff\") format(\"woff\"), url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.ttf\") format(\"truetype\"), url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.svg#B Lotus\") format(\"svg\"); }\n",
    "\n",
    "\n",
    "</style>\n",
    "\n",
    "<center style ='font-family: \"B Lotus\";'>بازیابی پیشرفته اطلاعات - درس پردازش زبان‌های طبیعی </center>\n",
    "<center style ='font-family: \"B Lotus\";'> آزمایشگاه پردازش هوشمند متن و زبان و علوم انسانی محاسباتی </center>\n",
    "<br>\n",
    "<center> http://language.ml </center>\n",
    "<center> contact: ehsan.asgari [AT] sharif [dot] edu </center>\n",
    "<center> semesters: Fall 2021 / Spring 2022 (Updated) </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "Examples:  ['he needs a car to get to work']\n",
      "Hypernym path:  [[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('instrumentality.n.03'), Synset('container.n.01'), Synset('wheeled_vehicle.n.01'), Synset('self-propelled_vehicle.n.01'), Synset('motor_vehicle.n.01'), Synset('car.n.01')], [Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('instrumentality.n.03'), Synset('conveyance.n.03'), Synset('vehicle.n.01'), Synset('wheeled_vehicle.n.01'), Synset('self-propelled_vehicle.n.01'), Synset('motor_vehicle.n.01'), Synset('car.n.01')]]\n",
      "Topic Domain:  []\n",
      "Holonyms:  []\n",
      "Meronems:  [Synset('accelerator.n.01'), Synset('air_bag.n.01'), Synset('auto_accessory.n.01'), Synset('automobile_engine.n.01'), Synset('automobile_horn.n.01'), Synset('buffer.n.06'), Synset('bumper.n.02'), Synset('car_door.n.01'), Synset('car_mirror.n.01'), Synset('car_seat.n.01'), Synset('car_window.n.01'), Synset('fender.n.01'), Synset('first_gear.n.01'), Synset('floorboard.n.02'), Synset('gasoline_engine.n.01'), Synset('glove_compartment.n.01'), Synset('grille.n.02'), Synset('high_gear.n.01'), Synset('hood.n.09'), Synset('luggage_compartment.n.01'), Synset('rear_window.n.01'), Synset('reverse.n.02'), Synset('roof.n.02'), Synset('running_board.n.01'), Synset('stabilizer_bar.n.01'), Synset('sunroof.n.01'), Synset('tail_fin.n.02'), Synset('third_gear.n.01'), Synset('window.n.02')]\n",
      "S-Holonyms:  []\n",
      "S-Meronems:  []\n",
      "M-Holonyms:  []\n",
      "M-Meronems:  []\n",
      "----------------------------------\n",
      "2 \t a wheeled vehicle adapted to the rails of railroad\n",
      "Examples:  ['three cars had jumped the rails']\n",
      "Hypernym path:  [[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('instrumentality.n.03'), Synset('container.n.01'), Synset('wheeled_vehicle.n.01'), Synset('car.n.02')], [Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('instrumentality.n.03'), Synset('conveyance.n.03'), Synset('vehicle.n.01'), Synset('wheeled_vehicle.n.01'), Synset('car.n.02')]]\n",
      "Topic Domain:  []\n",
      "Holonyms:  []\n",
      "Meronems:  [Synset('suspension.n.05')]\n",
      "S-Holonyms:  []\n",
      "S-Meronems:  []\n",
      "M-Holonyms:  [Synset('train.n.01')]\n",
      "M-Meronems:  []\n",
      "----------------------------------\n",
      "3 \t the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "Examples:  []\n",
      "Hypernym path:  [[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('structure.n.01'), Synset('area.n.05'), Synset('room.n.01'), Synset('compartment.n.02'), Synset('car.n.03')]]\n",
      "Topic Domain:  []\n",
      "Holonyms:  [Synset('airship.n.01')]\n",
      "Meronems:  []\n",
      "S-Holonyms:  []\n",
      "S-Meronems:  []\n",
      "M-Holonyms:  []\n",
      "M-Meronems:  []\n",
      "----------------------------------\n",
      "4 \t where passengers ride up and down\n",
      "Examples:  ['the car was on the top floor']\n",
      "Hypernym path:  [[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('structure.n.01'), Synset('area.n.05'), Synset('room.n.01'), Synset('compartment.n.02'), Synset('car.n.04')]]\n",
      "Topic Domain:  []\n",
      "Holonyms:  [Synset('elevator.n.01')]\n",
      "Meronems:  []\n",
      "S-Holonyms:  []\n",
      "S-Meronems:  []\n",
      "M-Holonyms:  []\n",
      "M-Meronems:  []\n",
      "----------------------------------\n",
      "5 \t a conveyance for passengers or freight on a cable railway\n",
      "Examples:  ['they took a cable car to the top of the mountain']\n",
      "Hypernym path:  [[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('structure.n.01'), Synset('area.n.05'), Synset('room.n.01'), Synset('compartment.n.02'), Synset('cable_car.n.01')]]\n",
      "Topic Domain:  []\n",
      "Holonyms:  [Synset('cable_railway.n.01')]\n",
      "Meronems:  []\n",
      "S-Holonyms:  []\n",
      "S-Meronems:  []\n",
      "M-Holonyms:  []\n",
      "M-Meronems:  []\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "synsets = wn.synsets('car', 'n')\n",
    "\n",
    "for idx, s in enumerate(synsets):\n",
    "    print(idx+1,'\\t' ,s.definition())\n",
    "    print('Examples: ', s.examples())\n",
    "    print('Hypernym path: ', s.hypernym_paths())\n",
    "    print('Topic Domain: ', s.topic_domains())        \n",
    "    print('Holonyms: ', s.part_holonyms())    \n",
    "    print('Meronems: ', s.part_meronyms())    \n",
    "    print('S-Holonyms: ', s.substance_holonyms())    \n",
    "    print('S-Meronems: ', s.substance_meronyms())    \n",
    "    print('M-Holonyms: ', s.member_holonyms())    \n",
    "    print('M-Meronems: ', s.member_meronyms())    \n",
    "    print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A)  [[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('thing.n.12'), Synset('part.n.03'), Synset('body_part.n.01'), Synset('external_body_part.n.01'), Synset('extremity.n.05'), Synset('hand.n.01')]]\n",
      "B)  [[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('thing.n.12'), Synset('part.n.03'), Synset('body_part.n.01'), Synset('external_body_part.n.01'), Synset('head.n.01')]]\n",
      "C)  [Synset('external_body_part.n.01')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Synset('person.n.01'), 'a human being')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synsets1 = wn.synsets('hand', 'n')\n",
    "\n",
    "synsets2 = wn.synsets('head', 'n')\n",
    "\n",
    "print('A) ', synsets1[0].hypernym_paths())\n",
    "\n",
    "print('B) ',synsets2[0].hypernym_paths())\n",
    "\n",
    "print('C) ',synsets1[0].lowest_common_hypernyms(synsets2[0]))\n",
    "\n",
    "ss = None\n",
    "ss_path = -1\n",
    "for x in synsets1:\n",
    "    for y in synsets2:\n",
    "        s = x.lowest_common_hypernyms(y)[0]\n",
    "        if len(s.hypernym_paths()) > ss_path:\n",
    "            ss = s\n",
    "            ss_path = len(s.hypernym_paths())\n",
    "\n",
    "ss, ss.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n NOUN\n",
    "#v VERB\n",
    "#a ADJECTIVE\n",
    "#s ADJECTIVE SATELLITE\n",
    "#r ADVERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('good.n.01')\n",
      "Synset('good.n.02')\n",
      "Synset('good.n.03')\n",
      "Synset('commodity.n.01')\n",
      "Synset('good.a.01')\n",
      "Synset('full.s.06')\n",
      "Synset('good.a.03')\n",
      "Synset('estimable.s.02')\n",
      "Synset('beneficial.s.01')\n",
      "Synset('good.s.06')\n",
      "Synset('good.s.07')\n",
      "Synset('adept.s.01')\n",
      "Synset('good.s.09')\n",
      "Synset('dear.s.02')\n",
      "Synset('dependable.s.04')\n",
      "Synset('good.s.12')\n",
      "Synset('good.s.13')\n",
      "Synset('effective.s.04')\n",
      "Synset('good.s.15')\n",
      "Synset('good.s.16')\n",
      "Synset('good.s.17')\n",
      "Synset('good.s.18')\n",
      "Synset('good.s.19')\n",
      "Synset('good.s.20')\n",
      "Synset('good.s.21')\n",
      "Synset('well.r.01')\n",
      "Synset('thoroughly.r.02')\n"
     ]
    }
   ],
   "source": [
    "for x in wn.synsets('good'):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Lesk Algorithm: WordNet Definitions for Similarity Comparison ##\n",
    "From http://pydoc.net/Python/nltk/3.0.0b2/nltk.wsd/# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Toolkit: Word Sense Disambiguation Algorithms\n",
    "#\n",
    "# Author: Liling Tan <alvations@gmail.com>\n",
    "#\n",
    "# Copyright (C) 2001-2014 NLTK Project\n",
    "# URL: <http://nltk.org/>\n",
    "# For license information, see LICENSE.TXT\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_overlaps_greedy(context, synsets_signatures, pos=None):\n",
    "    \"\"\"\n",
    "    Calculate overlaps between the context sentence and the synset_signature\n",
    "    and returns the synset with the highest overlap.\n",
    "    \n",
    "    :param context: ``context_sentence`` The context sentence where the ambiguous word occurs.\n",
    "    :param synsets_signatures: ``dictionary`` A list of words that 'signifies' the ambiguous word.\n",
    "    :param pos: ``pos`` A specified Part-of-Speech (POS).\n",
    "    :return: ``lesk_sense`` The Synset() object with the highest signature overlaps.\n",
    "    \"\"\"\n",
    "    max_overlaps = 0\n",
    "    lesk_sense = None\n",
    "    for ss in synsets_signatures:\n",
    "        if pos and str(ss.pos()) != pos: # Skips different POS.\n",
    "            continue\n",
    "        overlaps = set(synsets_signatures[ss]).intersection(context)\n",
    "        if len(overlaps) > max_overlaps:\n",
    "            lesk_sense = ss\n",
    "            max_overlaps = len(overlaps)  \n",
    "    return lesk_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(context_sentence, ambiguous_word, pos=None, dictionary=None):\n",
    "    \"\"\"\n",
    "    This function is the implementation of the original Lesk algorithm (1986).\n",
    "    It requires a dictionary which contains the definition of the different\n",
    "    sense of each word. See http://goo.gl/8TB15w\n",
    "\n",
    "        >>> from nltk import word_tokenize\n",
    "        >>> sent = word_tokenize(\"I went to the bank to deposit money.\")\n",
    "        >>> word = \"bank\"\n",
    "        >>> pos = \"n\"\n",
    "        >>> lesk(sent, word, pos)\n",
    "        Synset('bank.n.07')\n",
    "    \n",
    "    :param context_sentence: The context sentence where the ambiguous word occurs.\n",
    "    :param ambiguous_word: The ambiguous word that requires WSD.\n",
    "    :param pos: A specified Part-of-Speech (POS).\n",
    "    :param dictionary: A list of words that 'signifies' the ambiguous word.\n",
    "    :return: ``lesk_sense`` The Synset() object with the highest signature overlaps.\n",
    "    \"\"\"\n",
    "    if not dictionary:\n",
    "        dictionary = {}\n",
    "        for ss in wn.synsets(ambiguous_word):\n",
    "            dictionary[ss] = ss.definition().split()\n",
    "    best_sense = compare_overlaps_greedy(context_sentence, dictionary, pos)\n",
    "    return best_sense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('depository_financial_institution.n.01')\n",
      "Synset('deposit.v.02')\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = word_tokenize(\"I went to the bank to deposit money.\")\n",
    "\n",
    "print(lesk(sentence, \"bank\", \"n\"))\n",
    "\n",
    "print(lesk(sentence,\"deposit\", \"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sloping land (especially the slope beside a body of water)\n"
     ]
    }
   ],
   "source": [
    "sentence = word_tokenize(\"River bank failure can be caused when the gravitational forces.\")\n",
    "print(lesk(sentence, \"bank\", \"n\").definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
